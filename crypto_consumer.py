# import json
# import time
# from kafka import KafkaConsumer
# from elasticsearch import Elasticsearch
# from datetime import datetime
# import logging

# # Configuration du logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # Configuration Kafka
# KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']
# KAFKA_TOPIC = 'crypto-prices'

# # Configuration Elasticsearch
# ES_HOST = 'localhost:9200'
# ES_INDEX = 'crypto-prices'

# def create_elasticsearch_client():
#     """Cr√©er le client Elasticsearch"""
#     try:
#         es = Elasticsearch(
#             [f"http://{ES_HOST}"],
#             verify_certs=False,
#             basic_auth=None,
#             request_timeout=30
#         )
        
#         # Tester la connexion
#         if es.ping():
#             logger.info("‚úÖ Connexion Elasticsearch √©tablie")
#             return es
#         else:
#             logger.error("‚ùå Impossible de se connecter √† Elasticsearch")
#             return None
#     except Exception as e:
#         logger.error(f"‚ùå Erreur Elasticsearch: {e}")
#         return None


# def create_index_mapping(es):
#     """Cr√©er l'index avec mapping optimis√©"""
#     mapping = {
#         "mappings": {
#             "properties": {
#                 "crypto_id": {"type": "keyword"},
#                 "name": {"type": "text"},
#                 "symbol": {"type": "keyword"},
#                 "price_usd": {"type": "float"},
#                 "price_formatted": {"type": "keyword"},
#                 "market_cap_usd": {"type": "long"},
#                 "market_cap_formatted": {"type": "keyword"},
#                 "volume_24h": {"type": "long"},
#                 "price_change_24h": {"type": "float"},
#                 "trend": {"type": "keyword"},
#                 "timestamp": {"type": "date"},
#                 "@timestamp": {"type": "date"}
#             }
#         }
#     }
    
#     try:
#         if es.indices.exists(index=ES_INDEX):
#             logger.info(f"üìä Index '{ES_INDEX}' existe d√©j√†")
#         else:
#             es.indices.create(index=ES_INDEX, body=mapping)
#             logger.info(f"üéØ Index '{ES_INDEX}' cr√©√© avec succ√®s")
#     except Exception as e:
#         logger.error(f"‚ùå Erreur cr√©ation index: {e}")

# def process_message(es, message):
#     """Traiter et indexer un message"""
#     try:
#         # Parser le JSON
#         data = json.loads(message.value.decode('utf-8'))
        
#         # Ajouter timestamp Elasticsearch
#         data['@timestamp'] = datetime.utcnow().isoformat()
        
#         # Indexer dans Elasticsearch
#         doc_id = f"{data['crypto_id']}_{int(time.time())}"
        
#         es.index(
#             index=ES_INDEX,
#             id=doc_id,
#             body=data
#         )
        
#         logger.info(f"üìà Index√©: {data['name']} = {data['price_formatted']} {data['trend']}")
        
#     except Exception as e:
#         logger.error(f"‚ùå Erreur traitement message: {e}")

# def main():
#     """Fonction principale"""
#     logger.info("üöÄ D√©marrage du consumer Elasticsearch...")
    
#     # Cr√©er le client Elasticsearch
#     es = create_elasticsearch_client()
#     if not es:
#         return
    
#     # Cr√©er l'index
#     create_index_mapping(es)
    
#     # Cr√©er le consumer Kafka
#     try:
#         consumer = KafkaConsumer(
#             KAFKA_TOPIC,
#             bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
#             value_deserializer=lambda x: x,
#             auto_offset_reset='latest',  # Lire seulement les nouveaux messages
#             group_id='crypto-elasticsearch-consumer'
#         )
        
#         logger.info("‚úÖ Consumer Kafka cr√©√© avec succ√®s")
#         logger.info("‚è≥ En attente des messages...")
        
#         # Consommer les messages
#         for message in consumer:
#             process_message(es, message)
            
#     except KeyboardInterrupt:
#         logger.info("üõë Arr√™t du consumer")
#     except Exception as e:
#         logger.error(f"‚ùå Erreur consumer: {e}")
#     finally:
#         try:
#             consumer.close()
#         except:
#             pass

# if __name__ == "__main__":
#     main()
import json
import time
from kafka import KafkaConsumer
import requests
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration Kafka
KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']
KAFKA_TOPIC = 'crypto-prices'

def test_elasticsearch():
    """Tester Elasticsearch avec requests"""
    try:
        response = requests.get('http://localhost:9200/')
        if response.status_code == 200:
            logger.info("‚úÖ Elasticsearch accessible via HTTP")
            return True
        else:
            logger.error(f"‚ùå Elasticsearch erreur: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"‚ùå Erreur connexion: {e}")
        return False

def store_in_elasticsearch(data):
    """Stocker directement via API REST"""
    try:
        url = f"http://localhost:9200/crypto-prices/_doc"
        headers = {'Content-Type': 'application/json'}
        
        response = requests.post(url, json=data, headers=headers)
        
        if response.status_code in [200, 201]:
            # Utiliser get() pour √©viter les erreurs de cl√©s manquantes
            name = data.get('name', 'Unknown')
            price = data.get('price_formatted', 'N/A')
            trend = data.get('trend', '')
            
            logger.info(f"üìà Stock√©: {name} = {price} {trend}")
            return True
        else:
            logger.error(f"‚ùå Erreur stockage: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return False

def process_message(message):
    """Traiter un message Kafka"""
    try:
        # D√©coder le message JSON
        data = json.loads(message.value.decode('utf-8'))
        
        # Ajouter timestamp
        data['indexed_at'] = datetime.now().isoformat()
        
        # Stocker dans Elasticsearch
        store_in_elasticsearch(data)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur traitement: {e}")

def main():
    """Fonction principale"""
    logger.info("üöÄ D√©marrage du consumer...")
    
    # Tester Elasticsearch
    if not test_elasticsearch():
        logger.error("‚ùå Elasticsearch non accessible")
        return
    
    try:
        consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_deserializer=lambda x: x,
            auto_offset_reset='latest',
            group_id='crypto-elasticsearch-consumer'
        )
        
        logger.info("‚úÖ Consumer Kafka cr√©√©")
        logger.info("‚è≥ En attente des messages...")
        
        for message in consumer:
            process_message(message)
            
    except KeyboardInterrupt:
        logger.info("üõë Arr√™t du consumer")
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")

if __name__ == "__main__":
    from datetime import datetime
    main()
